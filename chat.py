import asyncio
import json
import os
import logging
import sys
import subprocess
from abc import ABC, abstractmethod

# ç»è¿‡éªŒè¯çš„åº“
import google.generativeai as genai
from google.generativeai.types import Tool as GeminiTool, FunctionDeclaration
from openai import OpenAI, AzureOpenAI
from dotenv import load_dotenv
from fastmcp import Client as MCPClient

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [BOT] - %(message)s')

# --- 1. æŠ½è±¡åŸºç±»å®šä¹‰ ---
class LLMHandler(ABC):
    """å®šä¹‰æ‰€æœ‰ LLM å¤„ç†å™¨å¿…é¡»éµå¾ªçš„æ¥å£ã€‚"""
    @abstractmethod
    def __init__(self, config: dict):
        pass

    @abstractmethod
    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list:
        """å°† MCP å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºç‰¹å®š LLM çš„æ ¼å¼ã€‚"""
        pass

    @abstractmethod
    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        """å¤„ç†å•ä¸ªè¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚"""
        pass

# --- 2. Gemini å®ç° ---
class GeminiHandler(LLMHandler):
    def __init__(self, config: dict):
        self.model_name = config.get("model_name")
        self.system_instruction = config.get("system_instruction")
        self.model = None

    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list[GeminiTool]:
        function_declarations = []
        for tool_summary in tool_summaries:
            input_schema = tool_summary.inputSchema
            properties = input_schema.get('properties', {})
            required = input_schema.get('required', [])
            for param_name, param_details in properties.items():
                if 'type' not in param_details:
                    param_details['type'] = 'string'
            func_decl = FunctionDeclaration(name=tool_summary.name, description=tool_summary.description,
                                            parameters={"type": "object", "properties": properties, "required": required})
            function_declarations.append(func_decl)
        return [GeminiTool(function_declarations=function_declarations)] if function_declarations else []

    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        if not self.model:
            print("æ­£åœ¨ä¸º Gemini è¿›è¡Œä¸€æ¬¡æ€§å·¥å…·å®šä¹‰æ£€æŸ¥...")
            gemini_tools = await get_initial_tool_schema(mcp_command, mcp_url, self.convert_mcp_tools_to_llm_format)
            if not gemini_tools:
                raise RuntimeError("æ— æ³•åœ¨å¯åŠ¨æ—¶è·å–å·¥å…·å®šä¹‰ã€‚")
            self.model = genai.GenerativeModel(model_name=self.model_name, tools=gemini_tools, system_instruction=self.system_instruction)
            print(f"âœ… Gemini æ¨¡å‹ '{self.model_name}' åˆå§‹åŒ–å®Œæˆã€‚")

        process = None
        try:
            logging.info(f"ä¸ºæ–°è¯·æ±‚å¯åŠ¨ Docker è¿›ç¨‹...")
            process = subprocess.Popen(mcp_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            await asyncio.sleep(5)

            async with MCPClient(mcp_url) as mcp_client:
                logging.info(f"âœ… æˆåŠŸè¿æ¥åˆ° MCP æœåŠ¡å™¨: {mcp_url}")
                chat = self.model.start_chat(history=chat_history)
                
                print("ğŸ¤” LLM æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
                response = await chat.send_message_async(user_input)

                while response.candidates and response.candidates[0].content.parts[0].function_call:
                    fc = response.candidates[0].content.parts[0].function_call
                    tool_name = fc.name
                    tool_args = {key: value for key, value in fc.args.items()}
                    logging.info(f"LLM è¯·æ±‚è°ƒç”¨å·¥å…·: {tool_name}ï¼Œå‚æ•°: {tool_args}")
                    tool_result = await mcp_client.call_tool(tool_name, tool_args)
                    logging.info(f"å·¥å…·è¿”å›ç»“æœ: {str(tool_result)[:300]}...")
                    
                    print("ğŸ¤” LLM æ­£åœ¨å¤„ç†å·¥å…·ç»“æœï¼Œè¯·ç¨å€™...")
                    tool_response_part = {"function_response": {"name": tool_name, "response": {"result": str(tool_result)}}}
                    response = await chat.send_message_async(tool_response_part)

                print(f"âœ¨ Gemini: {response.text}")
                chat_history.extend(chat.history)
        finally:
            if process:
                logging.info("æ­£åœ¨ç»ˆç»“æœ¬æ¬¡ä¼šè¯çš„ Docker è¿›ç¨‹...")
                process.terminate()
                process.wait()

# --- 3. OpenAI / Azure å®ç° ---
class OpenAIHandler(LLMHandler):
    def __init__(self, config: dict):
        provider = config.get("provider")
        if provider == "azure":
            self.client = AzureOpenAI(
                api_key=os.getenv("AZURE_OPENAI_KEY"),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
            )
            self.model_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
        else: # "openai"
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model_name = config.get("model_name")
        self.system_instruction = {"role": "system", "content": config.get("system_instruction")}
        self.tools = None

    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list:
        openai_tools = []
        for tool_summary in tool_summaries:
            input_schema = tool_summary.inputSchema
            properties = input_schema.get('properties', {})
            required = input_schema.get('required', [])
            for param_name, param_details in properties.items():
                if 'type' not in param_details:
                    param_details['type'] = 'string'
            
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool_summary.name,
                    "description": tool_summary.description,
                    "parameters": {"type": "object", "properties": properties, "required": required}
                }
            })
        return openai_tools

    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        if not self.tools:
            print(f"æ­£åœ¨ä¸º {self.client.__class__.__name__} è¿›è¡Œä¸€æ¬¡æ€§å·¥å…·å®šä¹‰æ£€æŸ¥...")
            self.tools = await get_initial_tool_schema(mcp_command, mcp_url, self.convert_mcp_tools_to_llm_format)
            if not self.tools:
                raise RuntimeError("æ— æ³•åœ¨å¯åŠ¨æ—¶è·å–å·¥å…·å®šä¹‰ã€‚")
            print(f"âœ… OpenAI/Azure æ¨¡å‹ '{self.model_name}' åˆå§‹åŒ–å®Œæˆã€‚")

        process = None
        try:
            logging.info(f"ä¸ºæ–°è¯·æ±‚å¯åŠ¨ Docker è¿›ç¨‹...")
            process = subprocess.Popen(mcp_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            await asyncio.sleep(5)

            async with MCPClient(mcp_url) as mcp_client:
                logging.info(f"âœ… æˆåŠŸè¿æ¥åˆ° MCP æœåŠ¡å™¨: {mcp_url}")
                
                messages = [self.system_instruction] + chat_history + [{"role": "user", "content": user_input}]
                
                while True:
                    print("ğŸ¤” LLM æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
                    response = self.client.chat.completions.create(model=self.model_name, messages=messages, tools=self.tools)
                    response_message = response.choices[0].message
                    
                    if not response_message.tool_calls:
                        final_text = response_message.content
                        print(f"âœ¨ OpenAI/Azure: {final_text}")
                        chat_history.append({"role": "user", "content": user_input})
                        chat_history.append({"role": "assistant", "content": final_text})
                        break
                    
                    messages.append(response_message)
                    for tool_call in response_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        logging.info(f"LLM è¯·æ±‚è°ƒç”¨å·¥å…·: {function_name}ï¼Œå‚æ•°: {function_args}")
                        
                        print("ğŸ¤” LLM æ­£åœ¨å¤„ç†å·¥å…·ç»“æœï¼Œè¯·ç¨å€™...")
                        tool_result = await mcp_client.call_tool(function_name, function_args)
                        logging.info(f"å·¥å…·è¿”å›ç»“æœ: {str(tool_result)[:300]}...")
                        
                        messages.append({
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": function_name,
                            "content": str(tool_result),
                        })
        finally:
            if process:
                logging.info("æ­£åœ¨ç»ˆç»“æœ¬æ¬¡ä¼šè¯çš„ Docker è¿›ç¨‹...")
                process.terminate()
                process.wait()

# --- 4. ä¸»ç¨‹åºé€»è¾‘ ---
def load_mcp_config():
    """åªä» config.json åŠ è½½ MCP æœåŠ¡å™¨é…ç½®ã€‚"""
    try:
        with open("config.json", 'r') as f:
            config = json.load(f)
        playwright_config = config["mcpServers"]["playwright"]
        command = [playwright_config["command"]] + playwright_config.get("args", [])
        url = playwright_config["url"]
        return command, url
    except Exception as e:
        logging.error(f"ğŸš¨ MCP é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None, None

async def main():
    """ä¸»ç¨‹åºï¼Œæ ¹æ®ä»£ç å†…è®¾ç½®é€‰æ‹©å¹¶è¿è¡Œ LLM å¤„ç†å™¨ã€‚"""
    
    # *** è¿™æ˜¯å…³é”®çš„ä¿®æ”¹ï¼šåœ¨è¿™é‡Œé€‰æ‹©è¦ä½¿ç”¨çš„ LLM æä¾›å•† ***
    # å¯é€‰é¡¹: "gemini", "openai", "azure"
    LLM_PROVIDER_TO_USE = "gemini"
    
    # åŠ è½½ .env æ–‡ä»¶ä¸­çš„æ‰€æœ‰ç¯å¢ƒå˜é‡
    load_dotenv()

    mcp_command, mcp_url = load_mcp_config()
    if not mcp_command or not mcp_url:
        return

    system_instruction = (
        "ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œä½ çš„ä»»åŠ¡æ˜¯ä½¿ç”¨æä¾›çš„å·¥å…·æ¥æ§åˆ¶ä¸€ä¸ªç½‘ç»œæµè§ˆå™¨ï¼Œä»¥å®Œæˆç”¨æˆ·çš„è¯·æ±‚ã€‚"
        "è¯·ä»”ç»†åˆ†æç”¨æˆ·çš„éœ€æ±‚ï¼Œå¹¶æŒ‰é¡ºåºè°ƒç”¨ä¸€ä¸ªæˆ–å¤šä¸ªå·¥å…·æ¥è¾¾æˆç›®æ ‡ã€‚"
        "å¯¹äºç‰¹å®šä»»åŠ¡ï¼Œè¯·ä¼˜å…ˆé€‰æ‹©ç›´æ¥çš„ç½‘ç«™ã€‚ä¾‹å¦‚ï¼Œè¦æŸ¥è¯¢å¤©æ°”ï¼Œè¯·ç›´æ¥å¯¼èˆªåˆ°ä¸€ä¸ªå¤©æ°”ç½‘ç«™ï¼Œè€Œä¸æ˜¯åœ¨è°·æ­Œæœç´¢ã€‚"
        "ä½ çš„ç¬¬ä¸€æ­¥å‡ ä¹æ°¸è¿œæ˜¯è°ƒç”¨ 'browser_navigate' å·¥å…·ã€‚"
        "å¦‚æœé‡åˆ°ä»»ä½•æ— æ³•å¤„ç†çš„é¡µé¢ï¼ˆå¦‚'æˆ‘ä¸æ˜¯æœºå™¨äºº'éªŒè¯ç ï¼‰ï¼Œè¯·æŠ¥å‘Šè¿™ä¸ªé—®é¢˜å¹¶åœæ­¢å½“å‰ä»»åŠ¡ï¼Œè€Œä¸æ˜¯å°è¯•ä¸ä¹‹äº¤äº’ã€‚"
    )

    # åœ¨ä»£ç ä¸­å®šä¹‰ LLM é…ç½®
    llm_configs = {
        "gemini": {"model_name": "gemini-2.5-flash"},
        "openai": {"model_name": "gpt-4-turbo"},
        "azure": {} # Azure çš„æ¨¡å‹åç§°ç›´æ¥ä» .env è¯»å–
    }
    
    current_llm_config = llm_configs.get(LLM_PROVIDER_TO_USE, {})
    current_llm_config["system_instruction"] = system_instruction
    current_llm_config["provider"] = LLM_PROVIDER_TO_USE

    handler: LLMHandler
    if LLM_PROVIDER_TO_USE == "gemini":
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        handler = GeminiHandler(current_llm_config)
    elif LLM_PROVIDER_TO_USE in ["openai", "azure"]:
        handler = OpenAIHandler(current_llm_config)
    else:
        logging.error(f"æœªçŸ¥çš„ LLM æä¾›å•†: '{LLM_PROVIDER_TO_USE}'ã€‚è¯·åœ¨ä»£ç ä¸­é€‰æ‹© 'gemini', 'openai', æˆ– 'azure'ã€‚")
        return

    chat_history = []
    print(f"\n--- ğŸ¤– æµè§ˆå™¨æ§åˆ¶æœºå™¨äººå·²å°±ç»ª (æä¾›å•†: {LLM_PROVIDER_TO_USE.upper()}) ---")
    print("ç°åœ¨å¯ä»¥ç›´æ¥ä¸‹è¾¾æŒ‡ä»¤ã€‚")

    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            if not user_input:
                print("âš ï¸ è¯·è¾“å…¥å†…å®¹ï¼Œæˆ–ä½¿ç”¨ 'exit' é€€å‡ºã€‚")
                continue
            if user_input.lower() in ['exit', 'quit']:
                print("ğŸ‘‹ æ­£åœ¨å…³é—­...")
                break
            
            await handler.get_response(user_input, mcp_command, mcp_url, chat_history)
        
        except Exception as e:
            logging.error(f"ğŸš¨ æœ¬è½®å¯¹è¯å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            print("æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•é‡æ–°æé—®ï¼Œæˆ–ä½¿ç”¨ 'exit' é€€å‡ºã€‚")

async def get_initial_tool_schema(command: list, url: str, converter) -> list | None:
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä»…ç”¨äºåœ¨ç¨‹åºå¯åŠ¨æ—¶è·å–ä¸€æ¬¡å·¥å…·çš„ schemaã€‚"""
    process = None
    try:
        process = subprocess.Popen(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        await asyncio.sleep(8)
        async with MCPClient(url) as mcp_client:
            tool_summaries = await mcp_client.list_tools()
            return converter(tool_summaries)
    except Exception as e:
        logging.error(f"è·å–åˆå§‹å·¥å…·å®šä¹‰æ—¶å‡ºé”™: {e}")
        return None
    finally:
        if process:
            process.terminate()
            process.wait()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")