import asyncio
import json
import os
import logging
import sys
import subprocess
from abc import ABC, abstractmethod

# ç»è¿‡éªŒè¯çš„åº“
import google.generativeai as genai
from google.generativeai.types import Tool as GeminiTool, FunctionDeclaration
from openai import OpenAI, AzureOpenAI
from dotenv import load_dotenv
from fastmcp import Client as MCPClient

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - [BOT] - %(message)s')

# --- 1. æŠ½è±¡åŸºç±»å®šä¹‰ ---
class LLMHandler(ABC):
    """å®šä¹‰æ‰€æœ‰ LLM å¤„ç†å™¨å¿…é¡»éµå¾ªçš„æ¥å£ã€‚"""
    @abstractmethod
    def __init__(self, config: dict):
        pass

    @abstractmethod
    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list:
        """å°† MCP å·¥å…·åˆ—è¡¨è½¬æ¢ä¸ºç‰¹å®š LLM çš„æ ¼å¼ã€‚"""
        pass

    @abstractmethod
    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        """å¤„ç†å•ä¸ªè¯·æ±‚çš„å®Œæ•´ç”Ÿå‘½å‘¨æœŸã€‚"""
        pass

# --- 2. Gemini å®ç° ---
class GeminiHandler(LLMHandler):
    def __init__(self, config: dict):
        self.model_name = config.get("model_name")
        self.system_instruction = config.get("system_instruction")
        self.model = None

    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list[GeminiTool]:
        function_declarations = []
        for tool_summary in tool_summaries:
            input_schema = tool_summary.inputSchema
            properties = input_schema.get('properties', {})
            required = input_schema.get('required', [])
            for param_name, param_details in properties.items():
                if 'type' not in param_details:
                    param_details['type'] = 'string'
            func_decl = FunctionDeclaration(name=tool_summary.name, description=tool_summary.description,
                                            parameters={"type": "object", "properties": properties, "required": required})
            function_declarations.append(func_decl)
        return [GeminiTool(function_declarations=function_declarations)] if function_declarations else []

    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        if not self.model:
            print("æ­£åœ¨ä¸º Gemini è¿›è¡Œä¸€æ¬¡æ€§å·¥å…·å®šä¹‰æ£€æŸ¥...")
            gemini_tools = await get_initial_tool_schema(mcp_command, mcp_url, self.convert_mcp_tools_to_llm_format)
            if not gemini_tools:
                raise RuntimeError("æ— æ³•åœ¨å¯åŠ¨æ—¶è·å–å·¥å…·å®šä¹‰ã€‚")
            self.model = genai.GenerativeModel(model_name=self.model_name, tools=gemini_tools, system_instruction=self.system_instruction)
            print(f"âœ… Gemini æ¨¡å‹ '{self.model_name}' åˆå§‹åŒ–å®Œæˆã€‚")

        process = None
        try:
            logging.info(f"ä¸ºæ–°è¯·æ±‚å¯åŠ¨ Docker è¿›ç¨‹...")
            process = subprocess.Popen(mcp_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            await asyncio.sleep(5)

            async with MCPClient(mcp_url) as mcp_client:
                logging.info(f"âœ… æˆåŠŸè¿æ¥åˆ° MCP æœåŠ¡å™¨: {mcp_url}")
                chat = self.model.start_chat(history=chat_history)
                
                print("ğŸ¤” LLM æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
                response = await chat.send_message_async(user_input)

                while response.candidates and response.candidates[0].content.parts[0].function_call:
                    fc = response.candidates[0].content.parts[0].function_call
                    tool_name = fc.name
                    tool_args = {key: value for key, value in fc.args.items()}
                    logging.info(f"LLM è¯·æ±‚è°ƒç”¨å·¥å…·: {tool_name}ï¼Œå‚æ•°: {tool_args}")
                    tool_result = await mcp_client.call_tool(tool_name, tool_args)
                    logging.info(f"å·¥å…·è¿”å›ç»“æœ: {str(tool_result)[:300]}...")
                    
                    print("ğŸ¤” LLM æ­£åœ¨å¤„ç†å·¥å…·ç»“æœï¼Œè¯·ç¨å€™...")
                    tool_response_part = {"function_response": {"name": tool_name, "response": {"result": str(tool_result)}}}
                    response = await chat.send_message_async(tool_response_part)

                print(f"âœ¨ Gemini: {response.text}")
                chat_history.extend(chat.history)
        finally:
            if process:
                logging.info("æ­£åœ¨ç»ˆç»“æœ¬æ¬¡ä¼šè¯çš„ Docker è¿›ç¨‹...")
                process.terminate()
                process.wait()

# --- 3. OpenAI / Azure å®ç° ---
class OpenAIHandler(LLMHandler):
    def __init__(self, config: dict):
        provider = config.get("provider")
        if provider == "azure":
            self.client = AzureOpenAI(
                api_key=os.getenv("AZURE_OPENAI_KEY"),
                api_version=os.getenv("AZURE_OPENAI_API_VERSION"),
                azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT")
            )
            self.model_name = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")
        else: # "openai"
            self.client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            self.model_name = config.get("model_name")
        self.system_instruction = {"role": "system", "content": config.get("system_instruction")}
        self.tools = None

    def convert_mcp_tools_to_llm_format(self, tool_summaries: list) -> list:
        openai_tools = []
        for tool_summary in tool_summaries:
            input_schema = tool_summary.inputSchema
            properties = input_schema.get('properties', {})
            required = input_schema.get('required', [])
            for param_name, param_details in properties.items():
                if 'type' not in param_details:
                    param_details['type'] = 'string'
            
            openai_tools.append({
                "type": "function",
                "function": {
                    "name": tool_summary.name,
                    "description": tool_summary.description,
                    "parameters": {"type": "object", "properties": properties, "required": required}
                }
            })
        return openai_tools

    async def get_response(self, user_input: str, mcp_command: list, mcp_url: str, chat_history: list):
        if not self.tools:
            print(f"æ­£åœ¨ä¸º {self.client.__class__.__name__} è¿›è¡Œä¸€æ¬¡æ€§å·¥å…·å®šä¹‰æ£€æŸ¥...")
            self.tools = await get_initial_tool_schema(mcp_command, mcp_url, self.convert_mcp_tools_to_llm_format)
            if not self.tools:
                raise RuntimeError("æ— æ³•åœ¨å¯åŠ¨æ—¶è·å–å·¥å…·å®šä¹‰ã€‚")
            print(f"âœ… OpenAI/Azure æ¨¡å‹ '{self.model_name}' åˆå§‹åŒ–å®Œæˆã€‚")

        process = None
        try:
            logging.info(f"ä¸ºæ–°è¯·æ±‚å¯åŠ¨ Docker è¿›ç¨‹...")
            process = subprocess.Popen(mcp_command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
            await asyncio.sleep(5)

            async with MCPClient(mcp_url) as mcp_client:
                logging.info(f"âœ… æˆåŠŸè¿æ¥åˆ° MCP æœåŠ¡å™¨: {mcp_url}")
                
                messages = [self.system_instruction] + chat_history + [{"role": "user", "content": user_input}]
                
                while True:
                    print("ğŸ¤” LLM æ­£åœ¨æ€è€ƒä¸­ï¼Œè¯·ç¨å€™...")
                    response = self.client.chat.completions.create(model=self.model_name, messages=messages, tools=self.tools)
                    response_message = response.choices[0].message
                    
                    if not response_message.tool_calls:
                        final_text = response_message.content
                        print(f"âœ¨ OpenAI/Azure: {final_text}")
                        chat_history.append({"role": "user", "content": user_input})
                        chat_history.append({"role": "assistant", "content": final_text})
                        break
                    
                    messages.append(response_message)
                    for tool_call in response_message.tool_calls:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        logging.info(f"LLM è¯·æ±‚è°ƒç”¨å·¥å…·: {function_name}ï¼Œå‚æ•°: {function_args}")
                        
                        print("ğŸ¤” LLM æ­£åœ¨å¤„ç†å·¥å…·ç»“æœï¼Œè¯·ç¨å€™...")
                        tool_result = await mcp_client.call_tool(function_name, function_args)
                        logging.info(f"å·¥å…·è¿”å›ç»“æœ: {str(tool_result)[:300]}...")
                        
                        messages.append({
                            "tool_call_id": tool_call.id,
                            "role": "tool",
                            "name": function_name,
                            "content": str(tool_result),
                        })
        finally:
            if process:
                logging.info("æ­£åœ¨ç»ˆç»“æœ¬æ¬¡ä¼šè¯çš„ Docker è¿›ç¨‹...")
                process.terminate()
                process.wait()

# --- 4. ä¸»ç¨‹åºé€»è¾‘ ---
def load_mcp_config():
    """åªä» config.json åŠ è½½ MCP æœåŠ¡å™¨é…ç½®ã€‚"""
    try:
        with open("config.json", 'r') as f:
            config = json.load(f)
        playwright_config = config["mcpServers"]["playwright"]
        command = [playwright_config["command"]] + playwright_config.get("args", [])
        url = playwright_config["url"]
        return command, url
    except Exception as e:
        logging.error(f"ğŸš¨ MCP é…ç½®åŠ è½½å¤±è´¥: {e}")
        return None, None

async def main():
    """ä¸»ç¨‹åºï¼Œæ ¹æ®ä»£ç å†…è®¾ç½®é€‰æ‹©å¹¶è¿è¡Œ LLM å¤„ç†å™¨ã€‚"""
    
    # *** è¿™æ˜¯å…³é”®çš„ä¿®æ”¹ï¼šåœ¨è¿™é‡Œé€‰æ‹©è¦ä½¿ç”¨çš„ LLM æä¾›å•† ***
    # å¯é€‰é¡¹: "gemini", "openai", "azure"
    LLM_PROVIDER_TO_USE = "gemini"
    
    # åŠ è½½ .env æ–‡ä»¶ä¸­çš„æ‰€æœ‰ç¯å¢ƒå˜é‡
    load_dotenv()

    mcp_command, mcp_url = load_mcp_config()
    if not mcp_command or not mcp_url:
        return

    system_instruction = (
        "ä½ æ˜¯ä¸€ä½é¡¶çº§çš„ç½‘ç»œè‡ªåŠ¨åŒ–ä¸“å®¶ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç²¾ç¡®åœ°ä½¿ç”¨å·¥å…·é›†æ¥æ“ä½œä¸€ä¸ªçœŸå®çš„æµè§ˆå™¨ï¼Œä»¥å®Œæˆç”¨æˆ·çš„æŒ‡ä»¤ã€‚"
        "åœ¨æ‰§è¡Œä»»ä½•æ“ä½œä¹‹å‰ï¼Œè¯·å§‹ç»ˆéµå¾ªä»¥ä¸‹æ ¸å¿ƒåŸåˆ™å’Œå·¥ä½œæµç¨‹ã€‚"
        "## æ ¸å¿ƒåŸåˆ™"
        "1.  **è§‚å¯Ÿä¼˜å…ˆ (Observe First)**ï¼šåœ¨è¿›è¡Œä»»ä½•äº¤äº’ï¼ˆå¦‚ç‚¹å‡»ã€è¾“å…¥ï¼‰ä¹‹å‰ï¼Œå¿…é¡»å…ˆä½¿ç”¨ `browser_snapshot` å·¥å…·æ¥ç†è§£å½“å‰çš„é¡µé¢ç»“æ„å’Œå¯ç”¨å…ƒç´ ã€‚ä¸è¦åœ¨ç›²ç›®çš„æƒ…å†µä¸‹è¡ŒåŠ¨ã€‚"
        "2.  **ä»»åŠ¡åˆ†è§£ (Decomposition)**ï¼šå¯¹äºå¤æ‚çš„ç”¨æˆ·è¯·æ±‚ï¼ˆä¾‹å¦‚â€œé¢„è®¢ä¸€å¼ ä»Aåˆ°Bçš„æœºç¥¨â€ï¼‰ï¼Œå…ˆåœ¨å¿ƒä¸­æ„æ€ä¸€ä¸ªæ¸…æ™°çš„ã€åˆ†æ­¥éª¤çš„è®¡åˆ’ã€‚ä¾‹å¦‚ï¼š1. æ‰“å¼€è®¢ç¥¨ç½‘ç«™ -> 2. è¾“å…¥å‡ºå‘åœ° -> 3. è¾“å…¥ç›®çš„åœ° -> 4. é€‰æ‹©æ—¥æœŸ -> 5. ç‚¹å‡»æœç´¢ã€‚"
        "3.  **ç²¾å‡†å®šä½ (Precise Targeting)**ï¼šåœ¨è°ƒç”¨ `browser_click` æˆ– `browser_type` ç­‰äº¤äº’å·¥å…·æ—¶ï¼Œä¼˜å…ˆé€‰æ‹©å…·æœ‰å”¯ä¸€IDã€`data-testid` æˆ–å…¶ä»–ç¨³å®šå±æ€§çš„å…ƒç´ ã€‚å¦‚æœä¸è¡Œï¼Œå†è€ƒè™‘ä½¿ç”¨æ–‡æœ¬å†…å®¹æˆ–CSSé€‰æ‹©å™¨ï¼Œä½†è¦ç¡®ä¿å…¶ç‹¬ç‰¹æ€§ã€‚"
        "4.  **ä¸»åŠ¨ç­‰å¾… (Proactive Waiting)**ï¼šç°ä»£ç½‘é¡µæ˜¯åŠ¨æ€åŠ è½½çš„ã€‚åœ¨å°è¯•ä¸æŸä¸ªå…ƒç´ äº¤äº’ä¹‹å‰ï¼Œå¦‚æœæ€€ç–‘å®ƒä¸æ˜¯ç«‹å³å‡ºç°çš„ï¼Œè¯·å…ˆä½¿ç”¨ `browser_wait_for` ç­‰å¾…è¯¥å…ƒç´ å˜å¾—å¯è§æˆ–å¯äº¤äº’ã€‚è¿™èƒ½æå¤§æé«˜æˆåŠŸç‡ã€‚"
        "5.  **ç»“æœéªŒè¯ (Verify Results)**ï¼šæ¯æ¬¡æ‰§è¡Œå®Œä¸€ä¸ªå…³é”®åŠ¨ä½œï¼ˆå¦‚å¯¼èˆªã€ç‚¹å‡»ã€è¡¨å•æäº¤ï¼‰åï¼Œéƒ½è¦é€šè¿‡ `browser_snapshot` å†æ¬¡è§‚å¯Ÿé¡µé¢ï¼Œç¡®è®¤ä½ çš„æ“ä½œæ˜¯å¦è¾¾åˆ°äº†é¢„æœŸçš„æ•ˆæœï¼ˆä¾‹å¦‚ï¼Œæ˜¯å¦è·³è½¬åˆ°äº†æ–°é¡µé¢ï¼Œæ˜¯å¦å‡ºç°äº†æ–°çš„å…ƒç´ ï¼‰ã€‚"

        "## æ ‡å‡†å·¥ä½œæµç¨‹"
        "1.  **åˆ†æéœ€æ±‚**ï¼šä»”ç»†é˜…è¯»ç”¨æˆ·çš„æœ€ç»ˆç›®æ ‡ã€‚"
        "2.  **åˆå§‹å¯¼èˆª**ï¼šå¦‚æœå½“å‰ä¸åœ¨ç›®æ ‡ç½‘ç«™ï¼Œç¬¬ä¸€æ­¥åº”ä½¿ç”¨ `browser_navigate` å‰å¾€ã€‚å¯¹äºä¸ç¡®å®šçš„ä»»åŠ¡ï¼Œå¯¼èˆªåˆ°Googleç­‰æœç´¢å¼•æ“è¿›è¡Œåˆæ­¥æ¢ç´¢ã€‚"
        "3.  **è§‚å¯Ÿä¸è®¡åˆ’**ï¼šä½¿ç”¨ `browser_snapshot` æ•è·å½“å‰é¡µé¢ä¿¡æ¯ï¼Œå¹¶æ ¹æ®ä½ çš„ä»»åŠ¡åˆ†è§£è®¡åˆ’ï¼Œç¡®å®šä¸‹ä¸€æ­¥è¦äº¤äº’çš„å…ƒç´ ã€‚"
        "4.  **æ‰§è¡Œå•æ­¥**ï¼šè°ƒç”¨ä¸€ä¸ªå·¥å…·ï¼ˆå¦‚ `browser_click`, `browser_type`ï¼‰å®Œæˆè®¡åˆ’ä¸­çš„ä¸€æ­¥ã€‚"
        "5.  **éªŒè¯ä¸å¾ªç¯**ï¼šå†æ¬¡ä½¿ç”¨ `browser_snapshot` éªŒè¯ä¸Šä¸€æ­¥çš„ç»“æœã€‚å¦‚æœæˆåŠŸï¼Œåˆ™ç»§ç»­æ‰§è¡Œè®¡åˆ’çš„ä¸‹ä¸€æ­¥ï¼›å¦‚æœå¤±è´¥ï¼Œåˆ™è¿›å…¥ä¸‹é¢çš„â€œå¤±è´¥æ¢å¤â€æµç¨‹ã€‚"

        "## å¤±è´¥æ¢å¤ä¸ç‰¹æ®Šæƒ…å†µ"
        "- **å…ƒç´ æœªæ‰¾åˆ°**ï¼šå¦‚æœä½ çš„é€‰æ‹©å™¨æ‰¾ä¸åˆ°å…ƒç´ ï¼Œä¸è¦ç«‹å³æ”¾å¼ƒã€‚é¦–å…ˆï¼Œä½¿ç”¨ `browser_snapshot` æŸ¥çœ‹å½“å‰é¡µé¢æ˜¯å¦ç¬¦åˆé¢„æœŸã€‚é¡µé¢å¯èƒ½åŠ è½½ç¼“æ…¢ã€å¼¹å‡ºäº†å¯¹è¯æ¡†ï¼Œæˆ–è€…ä½ çš„ä¸Šä¸€æ­¥æ“ä½œå¤±è´¥äº†ã€‚æ ¹æ®è§‚å¯Ÿè°ƒæ•´ä½ çš„ç­–ç•¥ã€‚"
        "- **å¤„ç†å¼¹çª—**ï¼šå¦‚æœå‡ºç°æµè§ˆå™¨åŸç”Ÿå¼¹çª—ï¼ˆAlert, Confirm, Promptï¼‰ï¼Œè¯·ä½¿ç”¨ `browser_handle_dialog` å·¥å…·æ¥å¤„ç†ã€‚"
        "- **é‡åˆ°æ ¸å¿ƒéšœç¢ï¼ˆå¦‚éªŒè¯ç æˆ–ç™»å½•å¢™ï¼‰**ï¼š"
            "**1. ä¸è¦ç«‹å³æ”¾å¼ƒå¹¶å‘ç”¨æˆ·æŠ¥å‘Šå¤±è´¥ã€‚ä½ çš„é¦–è¦ä»»åŠ¡æ˜¯å¯»æ‰¾å®Œæˆç”¨æˆ·ç›®æ ‡çš„æ›¿ä»£è·¯å¾„ã€‚**"
            "**2. è¯†åˆ«éšœç¢ç±»å‹ï¼šè¿™æ˜¯ä¸€ä¸ªéªŒè¯ç ï¼ˆCAPTCHAï¼‰ï¼Ÿä¸€ä¸ªå¼ºåˆ¶ç™»å½•é¡µé¢ï¼Ÿè¿˜æ˜¯ä¸€ä¸ªä»˜è´¹å¢™ï¼Ÿ**"
            "**3. é‡‡å–è§„é¿ç­–ç•¥ï¼š**"
                "- **å¯¹äºæœç´¢å¼•æ“çš„éªŒè¯ç ï¼š** å¦‚æœåœ¨æŸä¸ªæœç´¢å¼•æ“ï¼ˆå¦‚ `google.com`ï¼‰ä¸Šé‡åˆ°éªŒè¯ç ï¼Œ**ç«‹å³æ”¾å¼ƒè¯¥ç½‘ç«™ï¼Œå¹¶åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæœç´¢å¼•æ“**ã€‚ä¾‹å¦‚ï¼Œå°è¯•ä½¿ç”¨ `bing.com`ã€`duckduckgo.com` æˆ– `baidu.com` æ¥æ‰§è¡Œç›¸åŒçš„æœç´¢ã€‚è¿™æ˜¯å¤„ç†æ­¤é—®é¢˜çš„é¦–é€‰ç­–ç•¥ã€‚"
                "- **å¯¹äºç‰¹å®šç½‘ç«™çš„éšœç¢ï¼š** å¦‚æœä»»åŠ¡æ˜¯è·å–æ–°é—»æˆ–ä¿¡æ¯ï¼Œè€Œç›®æ ‡ç½‘ç«™è¢«æŒ¡ä½ï¼Œå¯ä»¥å°è¯•å»æœç´¢å¼•æ“æœç´¢ç›¸åŒçš„ä¸»é¢˜ï¼Œå¯»æ‰¾å…¶ä»–å¯ä»¥æä¾›ç›¸ä¼¼ä¿¡æ¯ä¸”æ²¡æœ‰éšœç¢çš„æ–°é—»æ¥æºæˆ–ç½‘ç«™ã€‚"
            "**4. æœ€åçš„æ‰‹æ®µï¼š** åªæœ‰åœ¨å°è¯•äº†å¤šç§æ›¿ä»£ç½‘ç«™å’Œæ–¹æ³•ï¼ˆä¾‹å¦‚ï¼Œè‡³å°‘å°è¯•äº†1-2ä¸ªå…¶ä»–æœç´¢å¼•æ“æˆ–ä¿¡æ¯æ¥æºï¼‰åä»ç„¶å¤±è´¥æ—¶ï¼Œæ‰èƒ½å‘ç”¨æˆ·æŠ¥å‘Šæ­¤éšœç¢ï¼Œå¹¶è§£é‡Šä½ å·²ç»å°è¯•è¿‡çš„æ‰€æœ‰æ›¿ä»£æ–¹æ¡ˆã€‚"
    )


    # åœ¨ä»£ç ä¸­å®šä¹‰ LLM é…ç½®
    llm_configs = {
        "gemini": {"model_name": "gemini-2.5-flash"},
        "openai": {"model_name": "gpt-4-turbo"},
        "azure": {} # Azure çš„æ¨¡å‹åç§°ç›´æ¥ä» .env è¯»å–
    }
    
    current_llm_config = llm_configs.get(LLM_PROVIDER_TO_USE, {})
    current_llm_config["system_instruction"] = system_instruction
    current_llm_config["provider"] = LLM_PROVIDER_TO_USE

    handler: LLMHandler
    if LLM_PROVIDER_TO_USE == "gemini":
        genai.configure(api_key=os.getenv("GOOGLE_API_KEY"))
        handler = GeminiHandler(current_llm_config)
    elif LLM_PROVIDER_TO_USE in ["openai", "azure"]:
        handler = OpenAIHandler(current_llm_config)
    else:
        logging.error(f"æœªçŸ¥çš„ LLM æä¾›å•†: '{LLM_PROVIDER_TO_USE}'ã€‚è¯·åœ¨ä»£ç ä¸­é€‰æ‹© 'gemini', 'openai', æˆ– 'azure'ã€‚")
        return

    chat_history = []
    print(f"\n--- ğŸ¤– æµè§ˆå™¨æ§åˆ¶æœºå™¨äººå·²å°±ç»ª (æä¾›å•†: {LLM_PROVIDER_TO_USE.upper()}) ---")
    print("ç°åœ¨å¯ä»¥ç›´æ¥ä¸‹è¾¾æŒ‡ä»¤ã€‚")

    while True:
        try:
            user_input = input("\nğŸ‘¤ ä½ : ").strip()
            if not user_input:
                print("âš ï¸ è¯·è¾“å…¥å†…å®¹ï¼Œæˆ–ä½¿ç”¨ 'exit' é€€å‡ºã€‚")
                continue
            if user_input.lower() in ['exit', 'quit']:
                print("ğŸ‘‹ æ­£åœ¨å…³é—­...")
                break
            
            await handler.get_response(user_input, mcp_command, mcp_url, chat_history)
        
        except Exception as e:
            logging.error(f"ğŸš¨ æœ¬è½®å¯¹è¯å‡ºç°ä¸¥é‡é”™è¯¯: {e}")
            print("æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„è¯·æ±‚æ—¶é‡åˆ°äº†é—®é¢˜ã€‚è¯·å°è¯•é‡æ–°æé—®ï¼Œæˆ–ä½¿ç”¨ 'exit' é€€å‡ºã€‚")

async def get_initial_tool_schema(command: list, url: str, converter) -> list | None:
    """ä¸€ä¸ªè¾…åŠ©å‡½æ•°ï¼Œä»…ç”¨äºåœ¨ç¨‹åºå¯åŠ¨æ—¶è·å–ä¸€æ¬¡å·¥å…·çš„ schemaã€‚"""
    process = None
    try:
        process = subprocess.Popen(command, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
        await asyncio.sleep(8)
        async with MCPClient(url) as mcp_client:
            tool_summaries = await mcp_client.list_tools()
            return converter(tool_summaries)
    except Exception as e:
        logging.error(f"è·å–åˆå§‹å·¥å…·å®šä¹‰æ—¶å‡ºé”™: {e}")
        return None
    finally:
        if process:
            process.terminate()
            process.wait()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\nç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­ã€‚")